{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 827,
      "metadata": {
        "id": "3DZwQgubsMHX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import scipy\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 828,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeRqGHktsOBf",
        "outputId": "abb77b0e-3b42-4225-85e2-46eb72917b05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 829,
      "metadata": {
        "id": "EAFpvz2TsT8E"
      },
      "outputs": [],
      "source": [
        "# Hyper parameters\n",
        "num_classes = 5\n",
        "num_epochs = 300\n",
        "batch_size = 64\n",
        "# batch_size = 64\n",
        "learning_rate = 0.01\n",
        "\n",
        "sequence_length = 1000 # 1セグメントあたりのサンプル数 250Hz * 5s\n",
        "input_size = 1 # 特徴量の種類？\n",
        "hidden_size = 120 # 論文でいうhidden layerはこれ?\n",
        "num_layers = 2 # Two-layered lstm?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 830,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7hv5SKasack",
        "outputId": "5e6fce7c-3a3a-4b49-f8cc-8eb70e30aaae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/mnt/c/Users/grpro/workspace/grad_thesis/lstm_cnn_ensemble\n",
            "CNN.ipynb   LSTM_colab.ipynb\t README.md  env\n",
            "LSTM.ipynb  LSTM_tutorial.ipynb  data\t    output.png\n"
          ]
        }
      ],
      "source": [
        "!pwd\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 831,
      "metadata": {
        "id": "_cUKNjIKtDB-"
      },
      "outputs": [],
      "source": [
        "default_sample_rate = 2000\n",
        "down_sample_rate = 250\n",
        "ratio = default_sample_rate // down_sample_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 832,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OvebjSOsaYP",
        "outputId": "c68cb6ef-d1dd-40e2-e2b6-4a8a4cbe42c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "151899 1215199 168\n",
            "155587 1244699 172\n",
            "150337 1202699 167\n",
            "150768 1206149 167\n",
            "152524 1220199 169\n"
          ]
        }
      ],
      "source": [
        "# num_classes人分のdataframeを作る\n",
        "radar_frame_list = []\n",
        "scaler = MinMaxScaler((-1, 1)) # Min-Max Scaler -1~1\n",
        "\n",
        "for i in range(1, num_classes + 1):\n",
        "    wave_dem_2d = []\n",
        "    file_path = \"./data/radar_%02d.csv\" % i\n",
        "    radar_frame = pd.read_csv(file_path)\n",
        "    wave = radar_frame.to_numpy().flatten()\n",
        "    wave_dem = signal.decimate(wave, ratio)\n",
        "    for i in range(len(wave_dem)):\n",
        "      wave_dem_2d.append([wave_dem[i]])\n",
        "    # wave_dem_ts = torch.from_numpy(wave_dem.astype(np.float32)).clone() # tensorにする\n",
        "    print(i, len(wave), len(wave_dem_2d) // 900)\n",
        "\n",
        "    remaining = len(wave_dem_2d)\n",
        "    n = 0\n",
        "    n_stop = sequence_length\n",
        "    wave_segments = []\n",
        "\n",
        "    while n_stop < len(wave_dem_2d):\n",
        "        n_start = 0 + ((sequence_length - 1) - (350 - 1)) * n\n",
        "        n_stop = n_start + sequence_length\n",
        "        tmp = []\n",
        "        wave_segments.append(scaler.fit_transform(wave_dem_2d[n_start:n_stop]))\n",
        "        n += 1\n",
        "    \n",
        "    radar_frame_list.append(wave_segments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 833,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.4311903 ],\n",
              "       [-0.397682  ],\n",
              "       [-0.36072063],\n",
              "       [-0.32017558],\n",
              "       [-0.27964272],\n",
              "       [-0.23889709],\n",
              "       [-0.19975085],\n",
              "       [-0.16227468],\n",
              "       [-0.1265833 ],\n",
              "       [-0.09293633],\n",
              "       [-0.06044722],\n",
              "       [-0.02922139],\n",
              "       [ 0.00151319],\n",
              "       [ 0.03188227],\n",
              "       [ 0.06203263],\n",
              "       [ 0.09204485],\n",
              "       [ 0.1214989 ],\n",
              "       [ 0.15016363],\n",
              "       [ 0.17738266],\n",
              "       [ 0.20264883],\n",
              "       [ 0.22540391],\n",
              "       [ 0.24515136],\n",
              "       [ 0.26163516],\n",
              "       [ 0.27466233],\n",
              "       [ 0.28434434],\n",
              "       [ 0.29091749],\n",
              "       [ 0.29482184],\n",
              "       [ 0.29663487],\n",
              "       [ 0.2969817 ],\n",
              "       [ 0.29654303],\n",
              "       [ 0.29592123],\n",
              "       [ 0.29565188],\n",
              "       [ 0.29613066],\n",
              "       [ 0.29760649],\n",
              "       [ 0.30018929],\n",
              "       [ 0.30384166],\n",
              "       [ 0.30842755],\n",
              "       [ 0.31371988],\n",
              "       [ 0.31944587],\n",
              "       [ 0.32531512],\n",
              "       [ 0.33105008],\n",
              "       [ 0.33642158],\n",
              "       [ 0.34126118],\n",
              "       [ 0.34547798],\n",
              "       [ 0.34905185],\n",
              "       [ 0.3520246 ],\n",
              "       [ 0.35448415],\n",
              "       [ 0.3565433 ],\n",
              "       [ 0.35832316],\n",
              "       [ 0.35993345],\n",
              "       [ 0.36146159],\n",
              "       [ 0.36296504],\n",
              "       [ 0.36447085],\n",
              "       [ 0.36598259],\n",
              "       [ 0.36749104],\n",
              "       [ 0.36898944],\n",
              "       [ 0.37048712],\n",
              "       [ 0.37202147],\n",
              "       [ 0.37366423],\n",
              "       [ 0.37552014],\n",
              "       [ 0.3777171 ],\n",
              "       [ 0.38038883],\n",
              "       [ 0.38365575],\n",
              "       [ 0.38760678],\n",
              "       [ 0.39228396],\n",
              "       [ 0.39767122],\n",
              "       [ 0.40368956],\n",
              "       [ 0.4102014 ],\n",
              "       [ 0.41702411],\n",
              "       [ 0.42395043],\n",
              "       [ 0.43077257],\n",
              "       [ 0.43730691],\n",
              "       [ 0.44341655],\n",
              "       [ 0.44902875],\n",
              "       [ 0.45414382],\n",
              "       [ 0.45883252],\n",
              "       [ 0.46322219],\n",
              "       [ 0.46747606],\n",
              "       [ 0.47177168],\n",
              "       [ 0.47628153],\n",
              "       [ 0.48115608],\n",
              "       [ 0.48650811],\n",
              "       [ 0.49239779],\n",
              "       [ 0.49882101],\n",
              "       [ 0.50570488],\n",
              "       [ 0.51291482],\n",
              "       [ 0.52027576],\n",
              "       [ 0.52760647],\n",
              "       [ 0.53476246],\n",
              "       [ 0.54167999],\n",
              "       [ 0.54841147],\n",
              "       [ 0.55514176],\n",
              "       [ 0.56217732],\n",
              "       [ 0.56990679],\n",
              "       [ 0.5787394 ],\n",
              "       [ 0.58903176],\n",
              "       [ 0.60101487],\n",
              "       [ 0.61473152],\n",
              "       [ 0.62999266],\n",
              "       [ 0.64636025],\n",
              "       [ 0.66316108],\n",
              "       [ 0.67953089],\n",
              "       [ 0.69448481],\n",
              "       [ 0.70700726],\n",
              "       [ 0.71615252],\n",
              "       [ 0.72114569],\n",
              "       [ 0.72147254],\n",
              "       [ 0.71694821],\n",
              "       [ 0.70775755],\n",
              "       [ 0.69446341],\n",
              "       [ 0.67798098],\n",
              "       [ 0.659518  ],\n",
              "       [ 0.64048351],\n",
              "       [ 0.62237021],\n",
              "       [ 0.60661806],\n",
              "       [ 0.59446969],\n",
              "       [ 0.58683141],\n",
              "       [ 0.58415497],\n",
              "       [ 0.58635532],\n",
              "       [ 0.5927776 ],\n",
              "       [ 0.60222243],\n",
              "       [ 0.61303163],\n",
              "       [ 0.62322874],\n",
              "       [ 0.63070075],\n",
              "       [ 0.63340104],\n",
              "       [ 0.6295496 ],\n",
              "       [ 0.61780622],\n",
              "       [ 0.59739574],\n",
              "       [ 0.56817168],\n",
              "       [ 0.53061263],\n",
              "       [ 0.48575404],\n",
              "       [ 0.43506646],\n",
              "       [ 0.38029915],\n",
              "       [ 0.32331067],\n",
              "       [ 0.26590706],\n",
              "       [ 0.20970458],\n",
              "       [ 0.15602939],\n",
              "       [ 0.10585957],\n",
              "       [ 0.05980974],\n",
              "       [ 0.01815371],\n",
              "       [-0.01912371],\n",
              "       [-0.05225717],\n",
              "       [-0.08162485],\n",
              "       [-0.10767519],\n",
              "       [-0.13086347],\n",
              "       [-0.15160357],\n",
              "       [-0.17023904],\n",
              "       [-0.18703486],\n",
              "       [-0.20218743],\n",
              "       [-0.21584689],\n",
              "       [-0.22814461],\n",
              "       [-0.23921887],\n",
              "       [-0.24923374],\n",
              "       [-0.25838889],\n",
              "       [-0.26692074],\n",
              "       [-0.27509529],\n",
              "       [-0.28319378],\n",
              "       [-0.29149228],\n",
              "       [-0.30023687],\n",
              "       [-0.30961789],\n",
              "       [-0.31974818],\n",
              "       [-0.33065029],\n",
              "       [-0.34225508],\n",
              "       [-0.35441189],\n",
              "       [-0.36690819],\n",
              "       [-0.37949493],\n",
              "       [-0.39191302],\n",
              "       [-0.40391757],\n",
              "       [-0.41529804],\n",
              "       [-0.4258937 ],\n",
              "       [-0.43560416],\n",
              "       [-0.44439478],\n",
              "       [-0.45229622],\n",
              "       [-0.45939816],\n",
              "       [-0.46583781],\n",
              "       [-0.47178471],\n",
              "       [-0.47742391],\n",
              "       [-0.48293901],\n",
              "       [-0.48849521],\n",
              "       [-0.49422267],\n",
              "       [-0.50020138],\n",
              "       [-0.50644862],\n",
              "       [-0.51290923],\n",
              "       [-0.51944857],\n",
              "       [-0.52584948],\n",
              "       [-0.53181652],\n",
              "       [-0.53699189],\n",
              "       [-0.54098549],\n",
              "       [-0.54341746],\n",
              "       [-0.54396777],\n",
              "       [-0.5424249 ],\n",
              "       [-0.53872485],\n",
              "       [-0.53297277],\n",
              "       [-0.52544287],\n",
              "       [-0.51655661],\n",
              "       [-0.50684324],\n",
              "       [-0.49689002],\n",
              "       [-0.48729044],\n",
              "       [-0.47859712],\n",
              "       [-0.47128506],\n",
              "       [-0.46572886],\n",
              "       [-0.46219517],\n",
              "       [-0.46084914],\n",
              "       [-0.46176993],\n",
              "       [-0.4649688 ],\n",
              "       [-0.47040246],\n",
              "       [-0.47797693],\n",
              "       [-0.48754168],\n",
              "       [-0.49887804],\n",
              "       [-0.5116882 ],\n",
              "       [-0.52559109],\n",
              "       [-0.540129  ],\n",
              "       [-0.55478566],\n",
              "       [-0.56901374],\n",
              "       [-0.5822692 ],\n",
              "       [-0.59404883],\n",
              "       [-0.60392699],\n",
              "       [-0.61158731],\n",
              "       [-0.61684507],\n",
              "       [-0.61965638],\n",
              "       [-0.62011218],\n",
              "       [-0.61841792],\n",
              "       [-0.61486259],\n",
              "       [-0.60978309],\n",
              "       [-0.60352981],\n",
              "       [-0.59643711],\n",
              "       [-0.58879987],\n",
              "       [-0.58085548],\n",
              "       [-0.5727716 ],\n",
              "       [-0.56464072],\n",
              "       [-0.55648295],\n",
              "       [-0.54825676],\n",
              "       [-0.5398756 ],\n",
              "       [-0.53122782],\n",
              "       [-0.52219753],\n",
              "       [-0.51268393],\n",
              "       [-0.50261674],\n",
              "       [-0.49196508],\n",
              "       [-0.48073797],\n",
              "       [-0.46897667],\n",
              "       [-0.45674083],\n",
              "       [-0.44409245],\n",
              "       [-0.43108134],\n",
              "       [-0.41773563],\n",
              "       [-0.40405998],\n",
              "       [-0.39004235],\n",
              "       [-0.37566889],\n",
              "       [-0.36094388],\n",
              "       [-0.34591006],\n",
              "       [-0.33066379],\n",
              "       [-0.31536055],\n",
              "       [-0.30020955],\n",
              "       [-0.2854584 ],\n",
              "       [-0.27136997],\n",
              "       [-0.25819451],\n",
              "       [-0.24614103],\n",
              "       [-0.23535339],\n",
              "       [-0.22589536],\n",
              "       [-0.21774712],\n",
              "       [-0.21081336],\n",
              "       [-0.20494112],\n",
              "       [-0.19994429],\n",
              "       [-0.19563106],\n",
              "       [-0.19183007],\n",
              "       [-0.1884107 ],\n",
              "       [-0.18529402],\n",
              "       [-0.18245261],\n",
              "       [-0.17990067],\n",
              "       [-0.17767713],\n",
              "       [-0.1758246 ],\n",
              "       [-0.17436655],\n",
              "       [-0.17328579],\n",
              "       [-0.17250772],\n",
              "       [-0.17189106],\n",
              "       [-0.17122761],\n",
              "       [-0.17025211],\n",
              "       [-0.16866139],\n",
              "       [-0.16614023],\n",
              "       [-0.16239063],\n",
              "       [-0.15716172],\n",
              "       [-0.15027726],\n",
              "       [-0.1416579 ],\n",
              "       [-0.13133575],\n",
              "       [-0.11945949],\n",
              "       [-0.10628902],\n",
              "       [-0.09218026],\n",
              "       [-0.07756098],\n",
              "       [-0.06289979],\n",
              "       [-0.0486705 ],\n",
              "       [-0.03531519],\n",
              "       [-0.02320927],\n",
              "       [-0.01263195],\n",
              "       [-0.00374512],\n",
              "       [ 0.00341691],\n",
              "       [ 0.00894553],\n",
              "       [ 0.01304385],\n",
              "       [ 0.01600225],\n",
              "       [ 0.01816675],\n",
              "       [ 0.01990438],\n",
              "       [ 0.02156978],\n",
              "       [ 0.02347698],\n",
              "       [ 0.02588003],\n",
              "       [ 0.02896508],\n",
              "       [ 0.03285424],\n",
              "       [ 0.03762015],\n",
              "       [ 0.04330942],\n",
              "       [ 0.04997189],\n",
              "       [ 0.05769185],\n",
              "       [ 0.06661628],\n",
              "       [ 0.07697493],\n",
              "       [ 0.08908872],\n",
              "       [ 0.10336382],\n",
              "       [ 0.1202689 ],\n",
              "       [ 0.14029388],\n",
              "       [ 0.16389058],\n",
              "       [ 0.19139991],\n",
              "       [ 0.22297325],\n",
              "       [ 0.25849973],\n",
              "       [ 0.29755316],\n",
              "       [ 0.33937124],\n",
              "       [ 0.38287595],\n",
              "       [ 0.42673768],\n",
              "       [ 0.4694777 ],\n",
              "       [ 0.50959738],\n",
              "       [ 0.54571984],\n",
              "       [ 0.57672863],\n",
              "       [ 0.60188711],\n",
              "       [ 0.62092409],\n",
              "       [ 0.63407456],\n",
              "       [ 0.64206879],\n",
              "       [ 0.64606895],\n",
              "       [ 0.64755888],\n",
              "       [ 0.64819905],\n",
              "       [ 0.64966296],\n",
              "       [ 0.65347048],\n",
              "       [ 0.66083023],\n",
              "       [ 0.67250002],\n",
              "       [ 0.68867401],\n",
              "       [ 0.70890635],\n",
              "       [ 0.73208257],\n",
              "       [ 0.75644934],\n",
              "       [ 0.77971026],\n",
              "       [ 0.79918786],\n",
              "       [ 0.8120436 ],\n",
              "       [ 0.8155372 ],\n",
              "       [ 0.8072964 ],\n",
              "       [ 0.78556278],\n",
              "       [ 0.74938114],\n",
              "       [ 0.69870799],\n",
              "       [ 0.63442536],\n",
              "       [ 0.55825853],\n",
              "       [ 0.47260984],\n",
              "       [ 0.38033155],\n",
              "       [ 0.28446767],\n",
              "       [ 0.18799653],\n",
              "       [ 0.09360338],\n",
              "       [ 0.00350515],\n",
              "       [-0.08065913],\n",
              "       [-0.15786808],\n",
              "       [-0.22769191],\n",
              "       [-0.29021187],\n",
              "       [-0.34590187],\n",
              "       [-0.39549122],\n",
              "       [-0.43982766],\n",
              "       [-0.47975662],\n",
              "       [-0.51602804],\n",
              "       [-0.54923632],\n",
              "       [-0.57979484],\n",
              "       [-0.60794236],\n",
              "       [-0.63377496],\n",
              "       [-0.65729453],\n",
              "       [-0.67846326],\n",
              "       [-0.69725463],\n",
              "       [-0.71369342],\n",
              "       [-0.72788054],\n",
              "       [-0.7400015 ],\n",
              "       [-0.75031965],\n",
              "       [-0.75915672],\n",
              "       [-0.76686436],\n",
              "       [-0.7737914 ],\n",
              "       [-0.78025205],\n",
              "       [-0.78650013],\n",
              "       [-0.79271334],\n",
              "       [-0.79898821],\n",
              "       [-0.80534391],\n",
              "       [-0.81173186],\n",
              "       [-0.81804818],\n",
              "       [-0.82414713],\n",
              "       [-0.82985455],\n",
              "       [-0.83498093],\n",
              "       [-0.8393343 ],\n",
              "       [-0.8427325 ],\n",
              "       [-0.84501472],\n",
              "       [-0.84605238],\n",
              "       [-0.84575918],\n",
              "       [-0.8440992 ],\n",
              "       [-0.84109152],\n",
              "       [-0.83680904],\n",
              "       [-0.83136873],\n",
              "       [-0.82491235],\n",
              "       [-0.81757926],\n",
              "       [-0.80947559],\n",
              "       [-0.80064517],\n",
              "       [-0.79104558],\n",
              "       [-0.78053277],\n",
              "       [-0.76885721],\n",
              "       [-0.75567504],\n",
              "       [-0.74057669],\n",
              "       [-0.72313377],\n",
              "       [-0.7029621 ],\n",
              "       [-0.67979515],\n",
              "       [-0.65355829],\n",
              "       [-0.62443005],\n",
              "       [-0.59287403],\n",
              "       [-0.55962736],\n",
              "       [-0.52563983],\n",
              "       [-0.4919709 ],\n",
              "       [-0.45966346],\n",
              "       [-0.42962001],\n",
              "       [-0.40250782],\n",
              "       [-0.37871413],\n",
              "       [-0.35836067],\n",
              "       [-0.34137204],\n",
              "       [-0.32757916],\n",
              "       [-0.31683042],\n",
              "       [-0.30908301],\n",
              "       [-0.30445285],\n",
              "       [-0.30321321],\n",
              "       [-0.30574402],\n",
              "       [-0.312444  ],\n",
              "       [-0.32362409],\n",
              "       [-0.33940245],\n",
              "       [-0.3596199 ],\n",
              "       [-0.3837893 ],\n",
              "       [-0.41108608],\n",
              "       [-0.4403803 ],\n",
              "       [-0.47030517],\n",
              "       [-0.49935352],\n",
              "       [-0.5259916 ],\n",
              "       [-0.54877821],\n",
              "       [-0.56647714],\n",
              "       [-0.57815172],\n",
              "       [-0.58323237],\n",
              "       [-0.58155058],\n",
              "       [-0.57333643],\n",
              "       [-0.55918043],\n",
              "       [-0.53996332],\n",
              "       [-0.51676044],\n",
              "       [-0.49072974],\n",
              "       [-0.46299445],\n",
              "       [-0.43453213],\n",
              "       [-0.40608135],\n",
              "       [-0.37807646],\n",
              "       [-0.35061821],\n",
              "       [-0.32348402],\n",
              "       [-0.29617663],\n",
              "       [-0.26800467],\n",
              "       [-0.23818449],\n",
              "       [-0.20594982],\n",
              "       [-0.17065421],\n",
              "       [-0.13185283],\n",
              "       [-0.08935372],\n",
              "       [-0.04323476],\n",
              "       [ 0.00617087],\n",
              "       [ 0.05831337],\n",
              "       [ 0.1124788 ],\n",
              "       [ 0.16784596],\n",
              "       [ 0.22354025],\n",
              "       [ 0.27867974],\n",
              "       [ 0.33241196],\n",
              "       [ 0.38394252],\n",
              "       [ 0.43255826],\n",
              "       [ 0.47764726],\n",
              "       [ 0.51871607],\n",
              "       [ 0.55540325],\n",
              "       [ 0.5874878 ],\n",
              "       [ 0.61489123],\n",
              "       [ 0.6376727 ],\n",
              "       [ 0.65601746],\n",
              "       [ 0.67021957],\n",
              "       [ 0.68065972],\n",
              "       [ 0.68777912],\n",
              "       [ 0.69205065],\n",
              "       [ 0.69394995],\n",
              "       [ 0.69393006],\n",
              "       [ 0.69240233],\n",
              "       [ 0.6897247 ],\n",
              "       [ 0.68619725],\n",
              "       [ 0.68206517],\n",
              "       [ 0.67752794],\n",
              "       [ 0.6727528 ],\n",
              "       [ 0.66788926],\n",
              "       [ 0.66308091],\n",
              "       [ 0.65847121],\n",
              "       [ 0.65420217],\n",
              "       [ 0.65040711],\n",
              "       [ 0.64720058],\n",
              "       [ 0.64467001],\n",
              "       [ 0.64287262],\n",
              "       [ 0.6418382 ],\n",
              "       [ 0.64157535],\n",
              "       [ 0.64207687],\n",
              "       [ 0.64332097],\n",
              "       [ 0.64526729],\n",
              "       [ 0.64784875],\n",
              "       [ 0.65096219],\n",
              "       [ 0.65446121],\n",
              "       [ 0.65815409],\n",
              "       [ 0.66180891],\n",
              "       [ 0.66516766],\n",
              "       [ 0.66797163],\n",
              "       [ 0.66999847],\n",
              "       [ 0.67110887],\n",
              "       [ 0.67129797],\n",
              "       [ 0.67074293],\n",
              "       [ 0.66983588],\n",
              "       [ 0.66919181],\n",
              "       [ 0.6696238 ],\n",
              "       [ 0.6720832 ],\n",
              "       [ 0.67756789],\n",
              "       [ 0.6870065 ],\n",
              "       [ 0.7011307 ],\n",
              "       [ 0.7203512 ],\n",
              "       [ 0.74465679],\n",
              "       [ 0.77355432],\n",
              "       [ 0.80606233],\n",
              "       [ 0.84076356],\n",
              "       [ 0.87591368],\n",
              "       [ 0.90959599],\n",
              "       [ 0.93990662],\n",
              "       [ 0.9651483 ],\n",
              "       [ 0.98400732],\n",
              "       [ 0.99568995],\n",
              "       [ 1.        ],\n",
              "       [ 0.99734817],\n",
              "       [ 0.98869404],\n",
              "       [ 0.97543281],\n",
              "       [ 0.95924538],\n",
              "       [ 0.94193216],\n",
              "       [ 0.92524803],\n",
              "       [ 0.91075045],\n",
              "       [ 0.89966582],\n",
              "       [ 0.89277158],\n",
              "       [ 0.89029259],\n",
              "       [ 0.89181763],\n",
              "       [ 0.89625072],\n",
              "       [ 0.90181607],\n",
              "       [ 0.90613376],\n",
              "       [ 0.90637517],\n",
              "       [ 0.89949473],\n",
              "       [ 0.88251923],\n",
              "       [ 0.85286267],\n",
              "       [ 0.80862565],\n",
              "       [ 0.74883672],\n",
              "       [ 0.67359945],\n",
              "       [ 0.58412155],\n",
              "       [ 0.48261999],\n",
              "       [ 0.37211475],\n",
              "       [ 0.25614018],\n",
              "       [ 0.13841367],\n",
              "       [ 0.02250603],\n",
              "       [-0.08844584],\n",
              "       [-0.191952  ],\n",
              "       [-0.28628992],\n",
              "       [-0.37052804],\n",
              "       [-0.44446229],\n",
              "       [-0.50848643],\n",
              "       [-0.56342321],\n",
              "       [-0.61034352],\n",
              "       [-0.65039778],\n",
              "       [-0.68467889],\n",
              "       [-0.71412897],\n",
              "       [-0.73949441],\n",
              "       [-0.76132642],\n",
              "       [-0.78001706],\n",
              "       [-0.79585626],\n",
              "       [-0.8090946 ],\n",
              "       [-0.81999965],\n",
              "       [-0.82889746],\n",
              "       [-0.83619445],\n",
              "       [-0.8423776 ],\n",
              "       [-0.84799433],\n",
              "       [-0.85361538],\n",
              "       [-0.85978636],\n",
              "       [-0.86697476],\n",
              "       [-0.87552012],\n",
              "       [-0.88559444],\n",
              "       [-0.89717886],\n",
              "       [-0.91006015],\n",
              "       [-0.92384878],\n",
              "       [-0.93801641],\n",
              "       [-0.95194835],\n",
              "       [-0.96500386],\n",
              "       [-0.97657775],\n",
              "       [-0.98615663],\n",
              "       [-0.99336388],\n",
              "       [-0.99798911],\n",
              "       [-1.        ],\n",
              "       [-0.99953662],\n",
              "       [-0.99689007],\n",
              "       [-0.99246826],\n",
              "       [-0.98675285],\n",
              "       [-0.98025225],\n",
              "       [-0.97345545],\n",
              "       [-0.96679132],\n",
              "       [-0.96059636],\n",
              "       [-0.95509308],\n",
              "       [-0.95037918],\n",
              "       [-0.9464263 ],\n",
              "       [-0.94308651],\n",
              "       [-0.94010381],\n",
              "       [-0.93712874],\n",
              "       [-0.93373517],\n",
              "       [-0.92944023],\n",
              "       [-0.92372903],\n",
              "       [-0.91608528],\n",
              "       [-0.90602748],\n",
              "       [-0.8931491 ],\n",
              "       [-0.87715957],\n",
              "       [-0.85792174],\n",
              "       [-0.83548049],\n",
              "       [-0.81007723],\n",
              "       [-0.78214594],\n",
              "       [-0.75228887],\n",
              "       [-0.72123422],\n",
              "       [-0.68978124],\n",
              "       [-0.65874125],\n",
              "       [-0.62888396],\n",
              "       [-0.60089691],\n",
              "       [-0.57536144],\n",
              "       [-0.5527436 ],\n",
              "       [-0.53339501],\n",
              "       [-0.51755729],\n",
              "       [-0.50536446],\n",
              "       [-0.49684068],\n",
              "       [-0.49189378],\n",
              "       [-0.49030817],\n",
              "       [-0.49174209],\n",
              "       [-0.49573408],\n",
              "       [-0.5017212 ],\n",
              "       [-0.50906819],\n",
              "       [-0.51710385],\n",
              "       [-0.52515931],\n",
              "       [-0.53260334],\n",
              "       [-0.53887103],\n",
              "       [-0.5434839 ],\n",
              "       [-0.54606058],\n",
              "       [-0.54631911],\n",
              "       [-0.54407328],\n",
              "       [-0.53922609],\n",
              "       [-0.53176224],\n",
              "       [-0.52174041],\n",
              "       [-0.50928542],\n",
              "       [-0.49457998],\n",
              "       [-0.4778552 ],\n",
              "       [-0.459379  ],\n",
              "       [-0.43944224],\n",
              "       [-0.41834306],\n",
              "       [-0.39637102],\n",
              "       [-0.37379324],\n",
              "       [-0.35084392],\n",
              "       [-0.32771782],\n",
              "       [-0.30456704],\n",
              "       [-0.28150041],\n",
              "       [-0.25858429],\n",
              "       [-0.2358443 ],\n",
              "       [-0.21326822],\n",
              "       [-0.1908108 ],\n",
              "       [-0.16840112],\n",
              "       [-0.14595296],\n",
              "       [-0.12337806],\n",
              "       [-0.10060159],\n",
              "       [-0.07757857],\n",
              "       [-0.05431015],\n",
              "       [-0.0308575 ],\n",
              "       [-0.00735153],\n",
              "       [ 0.01600354],\n",
              "       [ 0.03893355],\n",
              "       [ 0.06110591],\n",
              "       [ 0.08214948],\n",
              "       [ 0.10168223],\n",
              "       [ 0.11934491],\n",
              "       [ 0.13483733],\n",
              "       [ 0.14795399],\n",
              "       [ 0.15861448],\n",
              "       [ 0.16688326],\n",
              "       [ 0.17297418],\n",
              "       [ 0.17723733],\n",
              "       [ 0.18012919],\n",
              "       [ 0.1821699 ],\n",
              "       [ 0.18389365],\n",
              "       [ 0.18579916],\n",
              "       [ 0.18830689],\n",
              "       [ 0.19172825],\n",
              "       [ 0.19624986],\n",
              "       [ 0.20193364],\n",
              "       [ 0.20873059],\n",
              "       [ 0.21650503],\n",
              "       [ 0.22506512],\n",
              "       [ 0.23419464],\n",
              "       [ 0.24368018],\n",
              "       [ 0.25332894],\n",
              "       [ 0.26297457],\n",
              "       [ 0.27247317],\n",
              "       [ 0.28169531],\n",
              "       [ 0.29052087],\n",
              "       [ 0.29884014],\n",
              "       [ 0.30656052],\n",
              "       [ 0.31361447],\n",
              "       [ 0.31996388],\n",
              "       [ 0.32559672],\n",
              "       [ 0.3305153 ],\n",
              "       [ 0.33471925],\n",
              "       [ 0.33818921],\n",
              "       [ 0.34087713],\n",
              "       [ 0.34270789],\n",
              "       [ 0.34359382],\n",
              "       [ 0.34345992],\n",
              "       [ 0.34227445],\n",
              "       [ 0.34007864],\n",
              "       [ 0.33700908],\n",
              "       [ 0.33330714],\n",
              "       [ 0.32931233],\n",
              "       [ 0.32543876],\n",
              "       [ 0.32213574],\n",
              "       [ 0.31983452],\n",
              "       [ 0.31888581],\n",
              "       [ 0.31949516],\n",
              "       [ 0.32166682],\n",
              "       [ 0.32516614],\n",
              "       [ 0.32950782],\n",
              "       [ 0.33397428],\n",
              "       [ 0.33766659],\n",
              "       [ 0.33958616],\n",
              "       [ 0.33874065],\n",
              "       [ 0.33426396],\n",
              "       [ 0.3255372 ],\n",
              "       [ 0.3122948 ],\n",
              "       [ 0.29469913],\n",
              "       [ 0.27336909],\n",
              "       [ 0.24935475],\n",
              "       [ 0.2240591 ],\n",
              "       [ 0.19911508],\n",
              "       [ 0.17623354],\n",
              "       [ 0.15704317],\n",
              "       [ 0.14294595],\n",
              "       [ 0.135007  ],\n",
              "       [ 0.1338893 ],\n",
              "       [ 0.13983299],\n",
              "       [ 0.15266985],\n",
              "       [ 0.17185858],\n",
              "       [ 0.19652646],\n",
              "       [ 0.22550772],\n",
              "       [ 0.25737669],\n",
              "       [ 0.29048214],\n",
              "       [ 0.3229938 ],\n",
              "       [ 0.3529725 ],\n",
              "       [ 0.37846974],\n",
              "       [ 0.39765207],\n",
              "       [ 0.40893603],\n",
              "       [ 0.41111459],\n",
              "       [ 0.40345705],\n",
              "       [ 0.38576921],\n",
              "       [ 0.35840629],\n",
              "       [ 0.32223872],\n",
              "       [ 0.2785785 ],\n",
              "       [ 0.22907741],\n",
              "       [ 0.17560736],\n",
              "       [ 0.12013097],\n",
              "       [ 0.06457199],\n",
              "       [ 0.01069769],\n",
              "       [-0.039976  ],\n",
              "       [-0.08625018],\n",
              "       [-0.12727758],\n",
              "       [-0.16256693],\n",
              "       [-0.19196075],\n",
              "       [-0.2155931 ],\n",
              "       [-0.23383501],\n",
              "       [-0.24723522],\n",
              "       [-0.25646261],\n",
              "       [-0.26225401],\n",
              "       [-0.26537012],\n",
              "       [-0.26656018],\n",
              "       [-0.26653504],\n",
              "       [-0.26594811],\n",
              "       [-0.26538394],\n",
              "       [-0.26535327],\n",
              "       [-0.2662921 ],\n",
              "       [-0.26856223],\n",
              "       [-0.27245046],\n",
              "       [-0.27816517],\n",
              "       [-0.28583013],\n",
              "       [-0.29547734],\n",
              "       [-0.30704044],\n",
              "       [-0.3203498 ],\n",
              "       [-0.33513141],\n",
              "       [-0.35101182],\n",
              "       [-0.36753184],\n",
              "       [-0.38417132],\n",
              "       [-0.40038639],\n",
              "       [-0.41565773],\n",
              "       [-0.429544  ],\n",
              "       [-0.44173013],\n",
              "       [-0.45206087],\n",
              "       [-0.46055323],\n",
              "       [-0.46738514],\n",
              "       [-0.47286178],\n",
              "       [-0.4773664 ],\n",
              "       [-0.48130568],\n",
              "       [-0.48505922],\n",
              "       [-0.48893969],\n",
              "       [-0.49316539],\n",
              "       [-0.49784331],\n",
              "       [-0.50295975],\n",
              "       [-0.50837815],\n",
              "       [-0.51384607],\n",
              "       [-0.51901286],\n",
              "       [-0.52345764],\n",
              "       [-0.52672521],\n",
              "       [-0.52836601],\n",
              "       [-0.52797479],\n",
              "       [-0.52522201],\n",
              "       [-0.5198722 ],\n",
              "       [-0.51178586],\n",
              "       [-0.50090597],\n",
              "       [-0.48723619],\n",
              "       [-0.47082107],\n",
              "       [-0.4517394 ],\n",
              "       [-0.43011738],\n",
              "       [-0.40616107],\n",
              "       [-0.38019948],\n",
              "       [-0.35272346],\n",
              "       [-0.32440489],\n",
              "       [-0.29608537],\n",
              "       [-0.26873185],\n",
              "       [-0.2433649 ],\n",
              "       [-0.22097137],\n",
              "       [-0.20241698],\n",
              "       [-0.18837328],\n",
              "       [-0.17926885],\n",
              "       [-0.17526897],\n",
              "       [-0.17628489],\n",
              "       [-0.18200846],\n",
              "       [-0.19196354],\n",
              "       [-0.20556281],\n",
              "       [-0.22215996],\n",
              "       [-0.24108976],\n",
              "       [-0.26169366],\n",
              "       [-0.2833311 ],\n",
              "       [-0.30537979],\n",
              "       [-0.32722928],\n",
              "       [-0.34827172],\n",
              "       [-0.36789353],\n",
              "       [-0.38547141],\n",
              "       [-0.40037572],\n",
              "       [-0.41198351],\n",
              "       [-0.41970094],\n",
              "       [-0.42299168],\n",
              "       [-0.42140745],\n",
              "       [-0.41461747],\n",
              "       [-0.40243505],\n",
              "       [-0.38483969],\n",
              "       [-0.36199323],\n",
              "       [-0.33424774],\n",
              "       [-0.3021417 ],\n",
              "       [-0.26638183],\n",
              "       [-0.22781009],\n",
              "       [-0.18735762],\n",
              "       [-0.14598962],\n",
              "       [-0.10464545],\n",
              "       [-0.06417895],\n",
              "       [-0.02530244],\n",
              "       [ 0.01146209],\n",
              "       [ 0.04582022],\n",
              "       [ 0.07772553],\n",
              "       [ 0.10737695],\n",
              "       [ 0.13518904],\n",
              "       [ 0.16173423],\n",
              "       [ 0.18766049],\n",
              "       [ 0.21359301],\n",
              "       [ 0.24003274],\n",
              "       [ 0.26726696],\n",
              "       [ 0.29530715],\n",
              "       [ 0.32386506],\n",
              "       [ 0.35236967],\n",
              "       [ 0.38001985],\n",
              "       [ 0.40586218],\n",
              "       [ 0.42888191],\n",
              "       [ 0.44809643],\n",
              "       [ 0.46264243],\n",
              "       [ 0.47184923],\n",
              "       [ 0.47529105],\n",
              "       [ 0.47281283],\n",
              "       [ 0.4645287 ],\n",
              "       [ 0.45079652],\n",
              "       [ 0.43217667],\n",
              "       [ 0.40938375],\n",
              "       [ 0.38323872],\n",
              "       [ 0.35462575],\n",
              "       [ 0.32445643],\n",
              "       [ 0.29364197],\n",
              "       [ 0.26306982],\n",
              "       [ 0.23357926],\n",
              "       [ 0.20593206],\n",
              "       [ 0.18077901],\n",
              "       [ 0.15862672],\n",
              "       [ 0.13980862],\n",
              "       [ 0.1244637 ],\n",
              "       [ 0.11252847],\n",
              "       [ 0.10374763],\n",
              "       [ 0.09770561],\n",
              "       [ 0.09387687],\n",
              "       [ 0.09168805],\n",
              "       [ 0.0905805 ],\n",
              "       [ 0.09006025],\n",
              "       [ 0.0897271 ],\n",
              "       [ 0.08928283],\n",
              "       [ 0.08852552],\n",
              "       [ 0.08733961],\n",
              "       [ 0.08568789],\n",
              "       [ 0.08360524],\n",
              "       [ 0.08119085],\n",
              "       [ 0.07859606],\n",
              "       [ 0.07600676],\n",
              "       [ 0.07362068],\n",
              "       [ 0.07162128],\n",
              "       [ 0.07015165],\n",
              "       [ 0.06929052],\n",
              "       [ 0.06903334],\n",
              "       [ 0.06928176],\n",
              "       [ 0.06984526],\n",
              "       [ 0.07045682],\n",
              "       [ 0.070803  ],\n",
              "       [ 0.07056682],\n",
              "       [ 0.06948006],\n",
              "       [ 0.06737826],\n",
              "       [ 0.06424938],\n",
              "       [ 0.06026592],\n",
              "       [ 0.05578975],\n",
              "       [ 0.05134371],\n",
              "       [ 0.04755133],\n",
              "       [ 0.04505388],\n",
              "       [ 0.04441785],\n",
              "       [ 0.04604705],\n",
              "       [ 0.05011096],\n",
              "       [ 0.05649791],\n",
              "       [ 0.06479827],\n",
              "       [ 0.07432048],\n",
              "       [ 0.08414096],\n",
              "       [ 0.09318503],\n",
              "       [ 0.10033028],\n",
              "       [ 0.10452011],\n",
              "       [ 0.10487595],\n",
              "       [ 0.10079871],\n",
              "       [ 0.09205038],\n",
              "       [ 0.07880718],\n",
              "       [ 0.06167637],\n",
              "       [ 0.04167086],\n",
              "       [ 0.02013946],\n",
              "       [-0.00134292],\n",
              "       [-0.02111231],\n",
              "       [-0.03756274],\n",
              "       [-0.04928547],\n",
              "       [-0.05518451],\n",
              "       [-0.05455532],\n",
              "       [-0.04711858],\n",
              "       [-0.03300839],\n",
              "       [-0.01272475],\n",
              "       [ 0.01293236],\n",
              "       [ 0.04292756],\n",
              "       [ 0.07603743],\n",
              "       [ 0.11087548],\n",
              "       [ 0.14590083],\n",
              "       [ 0.17942648],\n",
              "       [ 0.20964664],\n",
              "       [ 0.23469831],\n",
              "       [ 0.25276174],\n",
              "       [ 0.26219278],\n",
              "       [ 0.26167043],\n",
              "       [ 0.25033752],\n",
              "       [ 0.22791329],\n",
              "       [ 0.19476052],\n",
              "       [ 0.15189635],\n",
              "       [ 0.10094198],\n",
              "       [ 0.04401496],\n",
              "       [-0.0164248 ],\n",
              "       [-0.07775912],\n",
              "       [-0.13740641],\n",
              "       [-0.19300738],\n",
              "       [-0.24258415],\n",
              "       [-0.28465839],\n",
              "       [-0.31831766],\n",
              "       [-0.34322479],\n",
              "       [-0.35957304],\n",
              "       [-0.3679972 ],\n",
              "       [-0.36945656],\n",
              "       [-0.36510726],\n",
              "       [-0.3561814 ],\n",
              "       [-0.34388614],\n",
              "       [-0.32933058]])"
            ]
          },
          "execution_count": 833,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "radar_frame_list[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 834,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ems1EUFIga7b",
        "outputId": "82458429-97d8-4211-f9cf-682888d4c20e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 834,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(wave_dem_2d) # tensorにはしていない"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 835,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnD1kKfnf4is",
        "outputId": "b51fc966-4476-47ae-9c1c-feaf04557ea0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-2.3435893667948337e-05"
            ]
          },
          "execution_count": 835,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wave_dem[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 836,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pic-ZwvTZi8X",
        "outputId": "e9352215-0aac-43ae-989d-76a8b0354912"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-2.37119380e-05]\n",
            " [-2.38183832e-05]\n",
            " [-2.39258871e-05]\n",
            " ...\n",
            " [-7.11852346e-06]\n",
            " [-6.73462954e-06]\n",
            " [-6.34069284e-06]]\n"
          ]
        }
      ],
      "source": [
        "print(radar_frame.to_numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 837,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1irmKjMtgTg",
        "outputId": "39c7fb2a-af6d-423d-a0a7-9d6d34e775ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 1, 1,  ..., 5, 5, 5])"
            ]
          },
          "execution_count": 837,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = []\n",
        "for i in range(len(radar_frame_list)):\n",
        "    for j in range(len(radar_frame_list[i])):\n",
        "        labels.append(i + 1)\n",
        "\n",
        "labels_df = pd.Series(labels)\n",
        "torch.tensor(labels_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 838,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYkmvxNStn92",
        "outputId": "0ad8d1ff-2f1e-409c-be86-6f17241bd36f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1170 425\n",
            "935 619\n",
            "703 838\n",
            "472 888\n",
            "233 450\n"
          ]
        }
      ],
      "source": [
        "df = pd.DataFrame(radar_frame_list)\n",
        "tmp = df.to_numpy().flatten()\n",
        "df = pd.Series(tmp).dropna()\n",
        "tmp = df.to_numpy().flatten()\n",
        "\n",
        "tmp_labels = labels_df.to_numpy().flatten()\n",
        "for i in reversed(range(len(tmp))):\n",
        "  if len(tmp[i]) != sequence_length:\n",
        "    print(i, len(tmp[i]))\n",
        "    tmp = np.delete(tmp, i)\n",
        "    tmp_labels = np.delete(tmp_labels, i)\n",
        "\n",
        "df = pd.Series(tmp)\n",
        "labels_df = pd.Series(tmp_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 839,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QJOJGa4ZOAL",
        "outputId": "32087f9f-5c11-42ec-8777-cb8612fa3445"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0       [[-0.43119030027867716], [-0.39768199524563536...\n",
              " 1       [[-0.8735282292357403], [-0.8559558009192504],...\n",
              " 2       [[-0.6323235077932776], [-0.6233832414220843],...\n",
              " 3       [[-0.7800916387927728], [-0.7991681872044234],...\n",
              " 4       [[0.17223992735245988], [0.19803851588219734],...\n",
              "                               ...                        \n",
              " 1161    [[0.35360439006574296], [0.37555350233418683],...\n",
              " 1162    [[0.14708770411253336], [0.1446543093132371], ...\n",
              " 1163    [[-0.1086875547518791], [-0.10866502525283459]...\n",
              " 1164    [[0.9191947403695329], [0.9420051599104073], [...\n",
              " 1165    [[0.512804350202705], [0.5158244517411077], [0...\n",
              " Length: 1166, dtype: object,\n",
              " pandas.core.series.Series,\n",
              " numpy.ndarray,\n",
              " numpy.ndarray,\n",
              " numpy.float64,\n",
              " 1000)"
            ]
          },
          "execution_count": 839,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df, type(df), type(df[0]), type(df[0][0]), type(df[0][0][0]), len(df[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 840,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ns6jKMK0jlMO",
        "outputId": "2f10620e-152d-4ebc-8f2f-411d062d318d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       1\n",
              "1       1\n",
              "2       1\n",
              "3       1\n",
              "4       1\n",
              "       ..\n",
              "1161    5\n",
              "1162    5\n",
              "1163    5\n",
              "1164    5\n",
              "1165    5\n",
              "Length: 1166, dtype: int64"
            ]
          },
          "execution_count": 840,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 841,
      "metadata": {
        "id": "8ALujHuBt48l"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, dataset, labels, root_dir, transform=None) -> None:\n",
        "        # super().__init__()\n",
        "        self.radar_heartbeat = dataset\n",
        "        self.labels = labels\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "          idx = idx.tolist()\n",
        "        # heartbeat_segment = torch.FloatTensor(self.radar_heartbeat[idx])\n",
        "        # subject_label = torch.LongTensor(self.labels[idx])\n",
        "        # if self.transform: # transform適用されず?\n",
        "        # heartbeat_segment = self.transform(self.radar_heartbeat[idx]) # tensorに\n",
        "        onehot_label = torch.eye(num_classes)[self.labels[idx] - 1] # one hot encording\n",
        "        # onehot_label = self.transform(self.labels[idx]) # tensorに\n",
        "\n",
        "        # return heartbeat_segment, subject_label\n",
        "        # return torch.from_numpy(self.radar_heartbeat.to_numpy().copy()), torch.tensor(labels_df)    \n",
        "        # return torch.stack(self.radar_heartbeat.to_numpy().tolist()), torch.tensor(labels_df)    \n",
        "        # return self.radar_heartbeat[idx], labels_df[idx]\n",
        "        return torch.tensor(self.radar_heartbeat[idx]), onehot_label\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.radar_heartbeat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 842,
      "metadata": {
        "id": "NZ90ssVrvahI"
      },
      "outputs": [],
      "source": [
        "dataset = MyDataset(df, labels_df, \"./data/\", transform=transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 843,
      "metadata": {
        "id": "-MY3ko2llOsV"
      },
      "outputs": [],
      "source": [
        "seg, lab = next(iter(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 844,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQ24_ehWml26",
        "outputId": "f388dded-0d1c-4362-f2d1-79f179885126"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Tensor, 1000, tensor([1., 0., 0., 0., 0.]))"
            ]
          },
          "execution_count": 844,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(seg), len(seg), lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 845,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOtIGfVluDBe",
        "outputId": "e66a3611-44ce-441f-e29d-03a1ec4dedc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "full: 1166 -> train: 932, test: 234\n"
          ]
        }
      ],
      "source": [
        "train_size = int(0.8 * len(df.values))\n",
        "test_size = len(df.values) - train_size\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "print(f\"full: {len(dataset)} -> train: {len(train_set)}, test: {len(test_set)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 846,
      "metadata": {
        "id": "KDkUwglxuHy4"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 847,
      "metadata": {
        "id": "cEr02Pd1uJya"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, num_classes) -> None:\n",
        "    super().__init__()\n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_size = hidden_size\n",
        "    # self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "    self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "    c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "    # print(x.size(), h0.size(), c0.size())\n",
        "    # out, _ = self.rnn(x, h0)\n",
        "    out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "    out = out[:, -1, :]\n",
        "\n",
        "    out = self.fc(out)\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 848,
      "metadata": {
        "id": "IDJNiVRAuLdH"
      },
      "outputs": [],
      "source": [
        "model = LSTM(input_size, hidden_size, num_layers, num_classes)\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 849,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVj9gCQnuMwa",
        "outputId": "2a417c7c-cb4e-43aa-f9f4-ae744ad75253"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_511/13283608.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  signals = torch.tensor(signals)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/`300], Step [30/30], Loss: 1.7956\n",
            "Epoch [2/`300], Step [30/30], Loss: 1.5694\n",
            "Epoch [3/`300], Step [30/30], Loss: 1.4702\n",
            "Epoch [4/`300], Step [30/30], Loss: 2.1177\n",
            "Epoch [5/`300], Step [30/30], Loss: 1.6322\n",
            "Epoch [6/`300], Step [30/30], Loss: 1.4918\n",
            "Epoch [7/`300], Step [30/30], Loss: 1.5190\n",
            "Epoch [8/`300], Step [30/30], Loss: 1.2990\n",
            "Epoch [9/`300], Step [30/30], Loss: 1.4224\n",
            "Epoch [10/`300], Step [30/30], Loss: 1.6376\n",
            "Epoch [11/`300], Step [30/30], Loss: 1.8575\n",
            "Epoch [12/`300], Step [30/30], Loss: 1.4125\n",
            "Epoch [13/`300], Step [30/30], Loss: 1.5525\n",
            "Epoch [14/`300], Step [30/30], Loss: 1.3632\n",
            "Epoch [15/`300], Step [30/30], Loss: 1.5081\n",
            "Epoch [16/`300], Step [30/30], Loss: 1.4916\n",
            "Epoch [17/`300], Step [30/30], Loss: 1.6327\n",
            "Epoch [18/`300], Step [30/30], Loss: 1.3919\n",
            "Epoch [19/`300], Step [30/30], Loss: 1.7424\n",
            "Epoch [20/`300], Step [30/30], Loss: 1.6839\n",
            "Epoch [21/`300], Step [30/30], Loss: 1.5574\n",
            "Epoch [22/`300], Step [30/30], Loss: 1.9009\n",
            "Epoch [23/`300], Step [30/30], Loss: 1.4153\n",
            "Epoch [24/`300], Step [30/30], Loss: 1.6799\n",
            "Epoch [25/`300], Step [30/30], Loss: 1.5381\n",
            "Epoch [26/`300], Step [30/30], Loss: 1.4763\n",
            "Epoch [27/`300], Step [30/30], Loss: 1.6286\n",
            "Epoch [28/`300], Step [30/30], Loss: 1.4830\n",
            "Epoch [29/`300], Step [30/30], Loss: 1.5503\n",
            "Epoch [30/`300], Step [30/30], Loss: 1.8831\n",
            "Epoch [31/`300], Step [30/30], Loss: 1.7006\n",
            "Epoch [32/`300], Step [30/30], Loss: 1.2009\n",
            "Epoch [33/`300], Step [30/30], Loss: 1.5288\n",
            "Epoch [34/`300], Step [30/30], Loss: 1.6307\n",
            "Epoch [35/`300], Step [30/30], Loss: 1.4137\n",
            "Epoch [36/`300], Step [30/30], Loss: 1.6453\n",
            "Epoch [37/`300], Step [30/30], Loss: 1.5716\n",
            "Epoch [38/`300], Step [30/30], Loss: 1.9389\n",
            "Epoch [39/`300], Step [30/30], Loss: 1.6057\n",
            "Epoch [40/`300], Step [30/30], Loss: 1.5819\n",
            "Epoch [41/`300], Step [30/30], Loss: 1.5821\n",
            "Epoch [42/`300], Step [30/30], Loss: 1.3023\n",
            "Epoch [43/`300], Step [30/30], Loss: 1.7419\n",
            "Epoch [44/`300], Step [30/30], Loss: 1.3630\n",
            "Epoch [45/`300], Step [30/30], Loss: 1.5099\n",
            "Epoch [46/`300], Step [30/30], Loss: 1.9645\n",
            "Epoch [47/`300], Step [30/30], Loss: 1.7027\n",
            "Epoch [48/`300], Step [30/30], Loss: 1.4078\n",
            "Epoch [49/`300], Step [30/30], Loss: 2.0293\n",
            "Epoch [50/`300], Step [30/30], Loss: 1.2327\n",
            "Epoch [51/`300], Step [30/30], Loss: 1.2376\n",
            "Epoch [52/`300], Step [30/30], Loss: 1.9570\n",
            "Epoch [53/`300], Step [30/30], Loss: 1.4710\n",
            "Epoch [54/`300], Step [30/30], Loss: 1.4394\n",
            "Epoch [55/`300], Step [30/30], Loss: 1.5676\n",
            "Epoch [56/`300], Step [30/30], Loss: 1.3680\n",
            "Epoch [57/`300], Step [30/30], Loss: 1.4879\n",
            "Epoch [58/`300], Step [30/30], Loss: 1.5366\n",
            "Epoch [59/`300], Step [30/30], Loss: 1.7059\n",
            "Epoch [60/`300], Step [30/30], Loss: 1.0835\n",
            "Epoch [61/`300], Step [30/30], Loss: 1.3232\n",
            "Epoch [62/`300], Step [30/30], Loss: 1.1742\n",
            "Epoch [63/`300], Step [30/30], Loss: 1.1696\n",
            "Epoch [64/`300], Step [30/30], Loss: 1.9174\n",
            "Epoch [65/`300], Step [30/30], Loss: 1.1892\n",
            "Epoch [66/`300], Step [30/30], Loss: 1.1875\n",
            "Epoch [67/`300], Step [30/30], Loss: 1.5836\n",
            "Epoch [68/`300], Step [30/30], Loss: 1.5013\n",
            "Epoch [69/`300], Step [30/30], Loss: 1.0637\n",
            "Epoch [70/`300], Step [30/30], Loss: 1.4818\n",
            "Epoch [71/`300], Step [30/30], Loss: 1.4147\n",
            "Epoch [72/`300], Step [30/30], Loss: 1.5151\n",
            "Epoch [73/`300], Step [30/30], Loss: 1.3157\n",
            "Epoch [74/`300], Step [30/30], Loss: 1.5527\n",
            "Epoch [75/`300], Step [30/30], Loss: 1.7769\n",
            "Epoch [76/`300], Step [30/30], Loss: 1.7241\n",
            "Epoch [77/`300], Step [30/30], Loss: 1.4602\n",
            "Epoch [78/`300], Step [30/30], Loss: 1.2842\n",
            "Epoch [79/`300], Step [30/30], Loss: 1.6386\n",
            "Epoch [80/`300], Step [30/30], Loss: 1.5060\n",
            "Epoch [81/`300], Step [30/30], Loss: 1.4404\n",
            "Epoch [82/`300], Step [30/30], Loss: 1.0554\n",
            "Epoch [83/`300], Step [30/30], Loss: 1.2629\n",
            "Epoch [84/`300], Step [30/30], Loss: 1.2543\n",
            "Epoch [85/`300], Step [30/30], Loss: 1.8360\n",
            "Epoch [86/`300], Step [30/30], Loss: 1.2891\n",
            "Epoch [87/`300], Step [30/30], Loss: 1.4474\n",
            "Epoch [88/`300], Step [30/30], Loss: 1.3850\n",
            "Epoch [89/`300], Step [30/30], Loss: 1.1106\n",
            "Epoch [90/`300], Step [30/30], Loss: 1.6575\n",
            "Epoch [91/`300], Step [30/30], Loss: 1.7018\n",
            "Epoch [92/`300], Step [30/30], Loss: 2.1382\n",
            "Epoch [93/`300], Step [30/30], Loss: 1.3419\n",
            "Epoch [94/`300], Step [30/30], Loss: 0.9813\n",
            "Epoch [95/`300], Step [30/30], Loss: 1.5375\n",
            "Epoch [96/`300], Step [30/30], Loss: 1.4377\n",
            "Epoch [97/`300], Step [30/30], Loss: 1.1952\n",
            "Epoch [98/`300], Step [30/30], Loss: 1.2391\n",
            "Epoch [99/`300], Step [30/30], Loss: 1.1914\n",
            "Epoch [100/`300], Step [30/30], Loss: 1.5117\n",
            "Epoch [101/`300], Step [30/30], Loss: 1.2080\n",
            "Epoch [102/`300], Step [30/30], Loss: 1.1583\n",
            "Epoch [103/`300], Step [30/30], Loss: 1.4902\n",
            "Epoch [104/`300], Step [30/30], Loss: 1.3919\n",
            "Epoch [105/`300], Step [30/30], Loss: 1.4928\n",
            "Epoch [106/`300], Step [30/30], Loss: 1.6933\n",
            "Epoch [107/`300], Step [30/30], Loss: 1.3406\n",
            "Epoch [108/`300], Step [30/30], Loss: 2.1998\n",
            "Epoch [109/`300], Step [30/30], Loss: 1.3389\n",
            "Epoch [110/`300], Step [30/30], Loss: 1.3321\n",
            "Epoch [111/`300], Step [30/30], Loss: 1.4025\n",
            "Epoch [112/`300], Step [30/30], Loss: 1.7587\n",
            "Epoch [113/`300], Step [30/30], Loss: 1.2120\n",
            "Epoch [114/`300], Step [30/30], Loss: 1.2911\n",
            "Epoch [115/`300], Step [30/30], Loss: 1.7427\n",
            "Epoch [116/`300], Step [30/30], Loss: 1.5310\n",
            "Epoch [117/`300], Step [30/30], Loss: 1.4618\n",
            "Epoch [118/`300], Step [30/30], Loss: 1.2414\n",
            "Epoch [119/`300], Step [30/30], Loss: 1.0579\n",
            "Epoch [120/`300], Step [30/30], Loss: 1.4177\n",
            "Epoch [121/`300], Step [30/30], Loss: 0.8977\n",
            "Epoch [122/`300], Step [30/30], Loss: 1.0214\n",
            "Epoch [123/`300], Step [30/30], Loss: 2.0122\n",
            "Epoch [124/`300], Step [30/30], Loss: 1.7788\n",
            "Epoch [125/`300], Step [30/30], Loss: 1.0466\n",
            "Epoch [126/`300], Step [30/30], Loss: 0.8877\n",
            "Epoch [127/`300], Step [30/30], Loss: 1.0732\n",
            "Epoch [128/`300], Step [30/30], Loss: 0.8863\n",
            "Epoch [129/`300], Step [30/30], Loss: 1.2889\n",
            "Epoch [130/`300], Step [30/30], Loss: 1.5328\n",
            "Epoch [131/`300], Step [30/30], Loss: 1.5351\n",
            "Epoch [132/`300], Step [30/30], Loss: 1.7752\n",
            "Epoch [133/`300], Step [30/30], Loss: 1.7763\n",
            "Epoch [134/`300], Step [30/30], Loss: 1.4822\n",
            "Epoch [135/`300], Step [30/30], Loss: 1.8698\n",
            "Epoch [136/`300], Step [30/30], Loss: 1.5463\n",
            "Epoch [137/`300], Step [30/30], Loss: 1.7858\n",
            "Epoch [138/`300], Step [30/30], Loss: 1.1718\n",
            "Epoch [139/`300], Step [30/30], Loss: 1.4375\n",
            "Epoch [140/`300], Step [30/30], Loss: 1.5468\n",
            "Epoch [141/`300], Step [30/30], Loss: 1.5063\n",
            "Epoch [142/`300], Step [30/30], Loss: 1.8452\n",
            "Epoch [143/`300], Step [30/30], Loss: 1.8275\n",
            "Epoch [144/`300], Step [30/30], Loss: 1.2698\n",
            "Epoch [145/`300], Step [30/30], Loss: 1.1744\n",
            "Epoch [146/`300], Step [30/30], Loss: 1.4308\n",
            "Epoch [147/`300], Step [30/30], Loss: 1.4956\n",
            "Epoch [148/`300], Step [30/30], Loss: 1.2267\n",
            "Epoch [149/`300], Step [30/30], Loss: 1.6663\n",
            "Epoch [150/`300], Step [30/30], Loss: 1.6152\n",
            "Epoch [151/`300], Step [30/30], Loss: 1.5105\n",
            "Epoch [152/`300], Step [30/30], Loss: 1.4206\n",
            "Epoch [153/`300], Step [30/30], Loss: 1.6458\n",
            "Epoch [154/`300], Step [30/30], Loss: 1.0879\n",
            "Epoch [155/`300], Step [30/30], Loss: 1.7095\n",
            "Epoch [156/`300], Step [30/30], Loss: 1.9837\n",
            "Epoch [157/`300], Step [30/30], Loss: 1.1928\n",
            "Epoch [158/`300], Step [30/30], Loss: 1.4130\n",
            "Epoch [159/`300], Step [30/30], Loss: 1.7284\n",
            "Epoch [160/`300], Step [30/30], Loss: 1.5905\n",
            "Epoch [161/`300], Step [30/30], Loss: 1.8009\n",
            "Epoch [162/`300], Step [30/30], Loss: 1.7254\n",
            "Epoch [163/`300], Step [30/30], Loss: 1.0243\n",
            "Epoch [164/`300], Step [30/30], Loss: 1.0141\n",
            "Epoch [165/`300], Step [30/30], Loss: 1.1940\n",
            "Epoch [166/`300], Step [30/30], Loss: 0.9052\n",
            "Epoch [167/`300], Step [30/30], Loss: 1.0681\n",
            "Epoch [168/`300], Step [30/30], Loss: 1.7736\n",
            "Epoch [169/`300], Step [30/30], Loss: 1.0398\n",
            "Epoch [170/`300], Step [30/30], Loss: 1.2153\n",
            "Epoch [171/`300], Step [30/30], Loss: 1.4921\n",
            "Epoch [172/`300], Step [30/30], Loss: 1.2763\n",
            "Epoch [173/`300], Step [30/30], Loss: 1.0417\n",
            "Epoch [174/`300], Step [30/30], Loss: 1.3367\n",
            "Epoch [175/`300], Step [30/30], Loss: 1.2147\n",
            "Epoch [176/`300], Step [30/30], Loss: 1.9272\n",
            "Epoch [177/`300], Step [30/30], Loss: 1.3307\n",
            "Epoch [178/`300], Step [30/30], Loss: 1.4513\n",
            "Epoch [179/`300], Step [30/30], Loss: 1.7148\n",
            "Epoch [180/`300], Step [30/30], Loss: 1.0779\n",
            "Epoch [181/`300], Step [30/30], Loss: 1.0156\n",
            "Epoch [182/`300], Step [30/30], Loss: 1.9228\n",
            "Epoch [183/`300], Step [30/30], Loss: 1.1881\n",
            "Epoch [184/`300], Step [30/30], Loss: 1.8077\n",
            "Epoch [185/`300], Step [30/30], Loss: 1.1949\n",
            "Epoch [186/`300], Step [30/30], Loss: 1.6322\n",
            "Epoch [187/`300], Step [30/30], Loss: 1.2819\n",
            "Epoch [188/`300], Step [30/30], Loss: 1.5548\n",
            "Epoch [189/`300], Step [30/30], Loss: 1.3184\n",
            "Epoch [190/`300], Step [30/30], Loss: 0.9709\n",
            "Epoch [191/`300], Step [30/30], Loss: 1.6803\n",
            "Epoch [192/`300], Step [30/30], Loss: 1.7230\n",
            "Epoch [193/`300], Step [30/30], Loss: 0.8911\n",
            "Epoch [194/`300], Step [30/30], Loss: 1.4806\n",
            "Epoch [195/`300], Step [30/30], Loss: 1.3215\n",
            "Epoch [196/`300], Step [30/30], Loss: 2.3314\n",
            "Epoch [197/`300], Step [30/30], Loss: 1.2221\n",
            "Epoch [198/`300], Step [30/30], Loss: 1.5378\n",
            "Epoch [199/`300], Step [30/30], Loss: 1.2518\n",
            "Epoch [200/`300], Step [30/30], Loss: 0.9532\n",
            "Epoch [201/`300], Step [30/30], Loss: 1.1958\n",
            "Epoch [202/`300], Step [30/30], Loss: 1.4698\n",
            "Epoch [203/`300], Step [30/30], Loss: 1.5015\n",
            "Epoch [204/`300], Step [30/30], Loss: 1.2706\n",
            "Epoch [205/`300], Step [30/30], Loss: 1.2584\n",
            "Epoch [206/`300], Step [30/30], Loss: 1.5860\n",
            "Epoch [207/`300], Step [30/30], Loss: 1.1940\n",
            "Epoch [208/`300], Step [30/30], Loss: 0.9299\n",
            "Epoch [209/`300], Step [30/30], Loss: 1.7404\n",
            "Epoch [210/`300], Step [30/30], Loss: 1.8150\n",
            "Epoch [211/`300], Step [30/30], Loss: 1.0239\n",
            "Epoch [212/`300], Step [30/30], Loss: 1.2248\n",
            "Epoch [213/`300], Step [30/30], Loss: 1.6690\n",
            "Epoch [214/`300], Step [30/30], Loss: 1.5540\n",
            "Epoch [215/`300], Step [30/30], Loss: 1.5493\n",
            "Epoch [216/`300], Step [30/30], Loss: 1.6178\n",
            "Epoch [217/`300], Step [30/30], Loss: 1.8233\n",
            "Epoch [218/`300], Step [30/30], Loss: 1.6145\n",
            "Epoch [219/`300], Step [30/30], Loss: 1.7051\n",
            "Epoch [220/`300], Step [30/30], Loss: 1.5841\n",
            "Epoch [221/`300], Step [30/30], Loss: 1.6169\n",
            "Epoch [222/`300], Step [30/30], Loss: 1.4933\n",
            "Epoch [223/`300], Step [30/30], Loss: 1.6624\n",
            "Epoch [224/`300], Step [30/30], Loss: 1.6268\n",
            "Epoch [225/`300], Step [30/30], Loss: 1.6294\n",
            "Epoch [226/`300], Step [30/30], Loss: 1.4764\n",
            "Epoch [227/`300], Step [30/30], Loss: 1.6179\n",
            "Epoch [228/`300], Step [30/30], Loss: 1.7316\n",
            "Epoch [229/`300], Step [30/30], Loss: 1.4584\n",
            "Epoch [230/`300], Step [30/30], Loss: 1.5646\n",
            "Epoch [231/`300], Step [30/30], Loss: 1.5322\n",
            "Epoch [232/`300], Step [30/30], Loss: 1.6884\n",
            "Epoch [233/`300], Step [30/30], Loss: 1.5859\n",
            "Epoch [234/`300], Step [30/30], Loss: 1.4003\n",
            "Epoch [235/`300], Step [30/30], Loss: 1.6504\n",
            "Epoch [236/`300], Step [30/30], Loss: 1.8940\n",
            "Epoch [237/`300], Step [30/30], Loss: 1.6002\n",
            "Epoch [238/`300], Step [30/30], Loss: 1.4772\n",
            "Epoch [239/`300], Step [30/30], Loss: 1.5784\n",
            "Epoch [240/`300], Step [30/30], Loss: 1.5194\n",
            "Epoch [241/`300], Step [30/30], Loss: 1.4249\n",
            "Epoch [242/`300], Step [30/30], Loss: 1.5439\n",
            "Epoch [243/`300], Step [30/30], Loss: 1.5949\n",
            "Epoch [244/`300], Step [30/30], Loss: 1.5333\n",
            "Epoch [245/`300], Step [30/30], Loss: 1.8077\n",
            "Epoch [246/`300], Step [30/30], Loss: 1.6428\n",
            "Epoch [247/`300], Step [30/30], Loss: 1.5970\n",
            "Epoch [248/`300], Step [30/30], Loss: 1.5528\n",
            "Epoch [249/`300], Step [30/30], Loss: 1.5341\n",
            "Epoch [250/`300], Step [30/30], Loss: 1.6110\n",
            "Epoch [251/`300], Step [30/30], Loss: 1.5916\n",
            "Epoch [252/`300], Step [30/30], Loss: 1.3594\n",
            "Epoch [253/`300], Step [30/30], Loss: 1.5672\n",
            "Epoch [254/`300], Step [30/30], Loss: 1.7102\n",
            "Epoch [255/`300], Step [30/30], Loss: 1.4603\n",
            "Epoch [256/`300], Step [30/30], Loss: 1.8821\n",
            "Epoch [257/`300], Step [30/30], Loss: 1.6512\n",
            "Epoch [258/`300], Step [30/30], Loss: 1.6572\n",
            "Epoch [259/`300], Step [30/30], Loss: 1.2248\n",
            "Epoch [260/`300], Step [30/30], Loss: 1.7070\n",
            "Epoch [261/`300], Step [30/30], Loss: 1.7286\n",
            "Epoch [262/`300], Step [30/30], Loss: 1.6770\n",
            "Epoch [263/`300], Step [30/30], Loss: 1.5865\n",
            "Epoch [264/`300], Step [30/30], Loss: 1.4262\n",
            "Epoch [265/`300], Step [30/30], Loss: 1.6201\n",
            "Epoch [266/`300], Step [30/30], Loss: 2.0160\n",
            "Epoch [267/`300], Step [30/30], Loss: 1.6867\n",
            "Epoch [268/`300], Step [30/30], Loss: 1.9642\n",
            "Epoch [269/`300], Step [30/30], Loss: 1.4938\n",
            "Epoch [270/`300], Step [30/30], Loss: 1.6257\n",
            "Epoch [271/`300], Step [30/30], Loss: 1.4492\n",
            "Epoch [272/`300], Step [30/30], Loss: 1.7676\n",
            "Epoch [273/`300], Step [30/30], Loss: 1.6457\n",
            "Epoch [274/`300], Step [30/30], Loss: 1.5869\n",
            "Epoch [275/`300], Step [30/30], Loss: 1.5872\n",
            "Epoch [276/`300], Step [30/30], Loss: 1.5109\n",
            "Epoch [277/`300], Step [30/30], Loss: 1.5907\n",
            "Epoch [278/`300], Step [30/30], Loss: 1.6862\n",
            "Epoch [279/`300], Step [30/30], Loss: 1.6225\n",
            "Epoch [280/`300], Step [30/30], Loss: 1.6070\n",
            "Epoch [281/`300], Step [30/30], Loss: 1.3591\n",
            "Epoch [282/`300], Step [30/30], Loss: 1.6370\n",
            "Epoch [283/`300], Step [30/30], Loss: 1.6025\n",
            "Epoch [284/`300], Step [30/30], Loss: 1.6672\n",
            "Epoch [285/`300], Step [30/30], Loss: 1.5277\n",
            "Epoch [286/`300], Step [30/30], Loss: 1.5043\n",
            "Epoch [287/`300], Step [30/30], Loss: 1.5781\n",
            "Epoch [288/`300], Step [30/30], Loss: 1.7076\n",
            "Epoch [289/`300], Step [30/30], Loss: 1.6030\n",
            "Epoch [290/`300], Step [30/30], Loss: 1.6734\n",
            "Epoch [291/`300], Step [30/30], Loss: 1.5958\n",
            "Epoch [292/`300], Step [30/30], Loss: 1.5308\n",
            "Epoch [293/`300], Step [30/30], Loss: 1.5858\n",
            "Epoch [294/`300], Step [30/30], Loss: 1.5158\n",
            "Epoch [295/`300], Step [30/30], Loss: 1.5198\n",
            "Epoch [296/`300], Step [30/30], Loss: 1.4976\n",
            "Epoch [297/`300], Step [30/30], Loss: 1.4405\n",
            "Epoch [298/`300], Step [30/30], Loss: 1.3440\n",
            "Epoch [299/`300], Step [30/30], Loss: 1.5536\n",
            "Epoch [300/`300], Step [30/30], Loss: 1.7032\n"
          ]
        }
      ],
      "source": [
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (signals, labels) in enumerate(train_loader):\n",
        "    signals = torch.tensor(signals)\n",
        "    signals = signals.float()\n",
        "    signals = signals.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # print(signals.size())\n",
        "    outputs = model(signals)\n",
        "    # print(outputs)\n",
        "    loss = criterion(outputs, labels) # will check the shapes of outputs and labels\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  print(f'Epoch [{epoch+1}/`{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 850,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14 / 32 = Acc: 43.75 %\n",
            "28 / 64 = Acc: 43.75 %\n",
            "41 / 96 = Acc: 42.708333333333336 %\n",
            "56 / 128 = Acc: 43.75 %\n",
            "73 / 160 = Acc: 45.625 %\n",
            "82 / 192 = Acc: 42.708333333333336 %\n",
            "91 / 224 = Acc: 40.625 %\n",
            "94 / 234 = Acc: 40.17094017094017 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_511/3918647096.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  signals = torch.tensor(signals)\n",
            "/tmp/ipykernel_511/3918647096.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  outputs[j] = softmax(out)\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  n_correct = 0\n",
        "  n_samples = 0\n",
        "  softmax = nn.Softmax()\n",
        "  for i, (signals, one_hot_labels) in enumerate(test_loader):\n",
        "    signals = torch.tensor(signals)\n",
        "    signals = signals.float()\n",
        "    signals = signals.to(device)\n",
        "    one_hot_labels = one_hot_labels.to(device)\n",
        "    # print(len(one_hot_labels))\n",
        "    outputs = model(signals)\n",
        "    for j, out in enumerate(outputs):\n",
        "      outputs[j] = softmax(out)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1) # predicted per batch size\n",
        "\n",
        "    n_samples += one_hot_labels.size(0) # add batch_size\n",
        "    # labels = [] # labels per batch size\n",
        "    for k, labels in enumerate(one_hot_labels):\n",
        "      # labels.append(torch.argmax(one_hot_labels[k]))\n",
        "      if predicted[k] == torch.argmax(labels):\n",
        "        # print(predicted[k], torch.argmax(labels[k]))\n",
        "        n_correct += 1\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'{n_correct} / {n_samples} = Acc: {acc} %')\n",
        "    # print(f'Accuracy of the network on the 10000 test images: {acc} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 851,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10])"
            ]
          },
          "execution_count": 851,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 852,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5])"
            ]
          },
          "execution_count": 852,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.argmax(labels)\n",
        "labels.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 853,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([0, 0, 0, 2, 1, 0, 0, 0, 1, 1], device='cuda:0'),\n",
              " tensor([1., 0., 0., 0., 0.], device='cuda:0'))"
            ]
          },
          "execution_count": 853,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted, (labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 854,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([ True,  True,  True, False, False,  True,  True,  True, False, False],\n",
              "        device='cuda:0'),\n",
              " tensor(0., device='cuda:0'))"
            ]
          },
          "execution_count": 854,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = (predicted == torch.argmax(labels[len(labels)-1]))\n",
        "result, labels[len(labels)-1]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 ('env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "509ebe440ff8829e001a14a6a100e0bf6e98d8654d4df1b14b1a4d7c455d9242"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
