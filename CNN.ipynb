{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet101, ResNet101_Weights\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet-101 + STFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 1024\n",
    "N = 10*fs\n",
    "nperseg = 512\n",
    "amp = 2 * np.sqrt(2)\n",
    "noise_power = 0.001 * fs / 2\n",
    "time = np.arange(N) / float(fs)\n",
    "carrier = amp * np.sin(2*np.pi*50*time)\n",
    "noise = np.random.normal(scale=np.sqrt(noise_power),\n",
    "                         size=time.shape)\n",
    "x = carrier + noise\n",
    "#Compute and plot the STFT’s magnitude.\n",
    "#STFTの振幅を計算してプロットします\n",
    "\n",
    "f, t, Zxx = signal.stft(x, fs=fs, nperseg=nperseg)\n",
    "plt.figure()\n",
    "plt.pcolormesh(t, f, np.abs(Zxx), vmin=0, vmax=amp)\n",
    "plt.ylim([f[1], f[-1]])\n",
    "plt.title('STFT Magnitude')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.yscale('log')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4001,), (3,), (4001, 3))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.shape, t.shape, Zxx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "weights = ResNet101_Weights.IMAGENET1K_V2\n",
    "model = resnet101(weights=weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "batch_size = 4\n",
    "epoch = 30\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "npersegを大きくすると周波数の分解能が上がるが時間の分解能が下がる．\n",
    "noverlapを大きくすると時間の分解能が上がる？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_frame = pd.read_csv('data/radar_09.csv')\n",
    "wave = radar_frame.to_numpy()\n",
    "wave = wave.flatten()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "wave_std = scaler.fit_transform(wave.reshape(-1, 1))\n",
    "wave_std = wave_std.flatten()\n",
    "\n",
    "fs = 2000\n",
    "k = 1\n",
    "nperseg = 1667 * k\n",
    "noverlap = 1667 * (k - 1) + 1200\n",
    "\n",
    "i = 12\n",
    "y_wave = wave_std[1667 * i:1667 * (i + 1)]\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(y_wave)), y_wave)\n",
    "\n",
    "f, t, Zxx = signal.stft(wave_std[1667 * i:1667 * (i + 1)], fs=fs, nperseg=nperseg, noverlap=noverlap, return_onesided=True)\n",
    "ax = plt.figure()\n",
    "\n",
    "plt.pcolormesh(t, f, np.abs(Zxx), vmin=np.min(np.abs(Zxx)), vmax=np.max(np.abs(Zxx)), shading=\"auto\")\n",
    "# plt.ylim(-15 - 10, -5 + 10)\n",
    "plt.ylim(0, 20)\n",
    "# np.abs(Zxx)\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全データに対してSTFTを行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./data/img/\"\n",
    "scaler = StandardScaler()\n",
    "\n",
    "fs = 2000\n",
    "k = 10\n",
    "nperseg = 1667 * k\n",
    "noverlap = 1667 * (k - 1) + 833\n",
    "\n",
    "for i in range(20, num_class + 1):\n",
    "    csv_path = \"./data/radar_%02d.csv\" % i\n",
    "    radar_frame = pd.read_csv(csv_path)\n",
    "    wave = radar_frame.to_numpy()\n",
    "    wave = wave.flatten()\n",
    "    wave_std = scaler.fit_transform(wave.reshape(-1, 1))\n",
    "    wave_std = wave_std.flatten()\n",
    "    k = 0\n",
    "\n",
    "    while 1667 * (k + 10) < len(wave_std):\n",
    "        f, t, Zxx = signal.stft(wave_std[1667 * k:1667 * (k + 10)], fs=fs, nperseg=nperseg, noverlap=noverlap, return_onesided=True)\n",
    "        plt.figure()\n",
    "        plt.pcolormesh(t, f, np.abs(Zxx), vmin=np.min(np.abs(Zxx)), vmax=np.max(np.abs(Zxx)), shading=\"auto\")\n",
    "        plt.ylim(0, 10)\n",
    "        img_name = \"stft_%02d_%03d.png\" % (i, k)\n",
    "        plt.savefig(save_path + img_name)\n",
    "        plt.close()\n",
    "        k += 10 # 10ずつずらす"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"./data/img/stft_01_000.png\").convert('RGB')\n",
    "inputs = transform(img)\n",
    "inputs = inputs.unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_sizeやmean, stdはデータに合わせて設定してください。\n",
    "image_size = 224\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "# trainデータとvalidationデータが入っているディレクトリのパスを指定\n",
    "train_image_dir = './data/img/train'\n",
    "val_image_dir = './data/img/val'\n",
    "\n",
    "# trainデータ向けとvalidationデータ向けに、transformを用意します。\n",
    "# 皆さんのやりたいことに合わせて適宜変更してください。\n",
    "data_transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(\n",
    "            image_size, scale=(0.5, 1.0)\n",
    "        ),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(degrees=[-15, 15]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "        transforms.RandomErasing(0.5),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "}\n",
    "\n",
    "# torchvision.datasets.ImageFolderでデータの入っているディレクトリのパスと\n",
    "# transformを指定してあげるだけ。\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=train_image_dir, transform=data_transform['train'])\n",
    "val_dataset = torchvision.datasets.ImageFolder(root=val_image_dir, transform=data_transform['val'])\n",
    "\n",
    "# Datasetができたら、dataloaderに渡してあげればOK\n",
    "batch_size = 8\n",
    "train_dataLoader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "val_dataLoader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "# 以降はモデルを設定して学習・・・（割愛）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 記録用リストの初期化\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], loss: 0.25838 acc: 0.20345 val_loss: 1.12323, val_acc: 0.19178\n",
      "Epoch [2/30], loss: 0.23657 acc: 0.23103 val_loss: 0.59434, val_acc: 0.21918\n",
      "Epoch [3/30], loss: 0.23634 acc: 0.15862 val_loss: 0.36101, val_acc: 0.20548\n",
      "Epoch [4/30], loss: 0.21948 acc: 0.21034 val_loss: 0.92598, val_acc: 0.21918\n",
      "Epoch [5/30], loss: 0.22153 acc: 0.18276 val_loss: 0.22009, val_acc: 0.20548\n",
      "Epoch [6/30], loss: 0.21830 acc: 0.19310 val_loss: 0.22785, val_acc: 0.20548\n",
      "Epoch [7/30], loss: 0.22149 acc: 0.16897 val_loss: 0.22016, val_acc: 0.19178\n",
      "Epoch [8/30], loss: 0.22074 acc: 0.21379 val_loss: 0.23106, val_acc: 0.20548\n",
      "Epoch [9/30], loss: 0.21958 acc: 0.17586 val_loss: 0.22001, val_acc: 0.21918\n",
      "Epoch [10/30], loss: 0.21961 acc: 0.17586 val_loss: 0.21990, val_acc: 0.20548\n",
      "Epoch [11/30], loss: 0.21874 acc: 0.18966 val_loss: 0.22034, val_acc: 0.19178\n",
      "Epoch [12/30], loss: 0.22534 acc: 0.17586 val_loss: 0.21960, val_acc: 0.20548\n",
      "Epoch [13/30], loss: 0.21945 acc: 0.20000 val_loss: 0.22129, val_acc: 0.19178\n",
      "Epoch [14/30], loss: 0.21171 acc: 0.18276 val_loss: 0.22081, val_acc: 0.19178\n",
      "Epoch [15/30], loss: 0.20789 acc: 0.23103 val_loss: 0.22102, val_acc: 0.19178\n",
      "Epoch [16/30], loss: 0.21157 acc: 0.17241 val_loss: 0.22095, val_acc: 0.12329\n",
      "Epoch [17/30], loss: 0.21504 acc: 0.22414 val_loss: 0.22071, val_acc: 0.19178\n",
      "Epoch [18/30], loss: 0.21641 acc: 0.21724 val_loss: 0.43479, val_acc: 0.20548\n",
      "Epoch [19/30], loss: 0.21310 acc: 0.21724 val_loss: 0.22027, val_acc: 0.20548\n",
      "Epoch [20/30], loss: 0.21026 acc: 0.21379 val_loss: 0.22034, val_acc: 0.21918\n",
      "Epoch [21/30], loss: 0.21516 acc: 0.16552 val_loss: 0.22052, val_acc: 0.21918\n",
      "Epoch [22/30], loss: 0.20860 acc: 0.15517 val_loss: 0.22108, val_acc: 0.19178\n",
      "Epoch [23/30], loss: 0.21170 acc: 0.21724 val_loss: 0.22096, val_acc: 0.19178\n",
      "Epoch [24/30], loss: 0.20954 acc: 0.21034 val_loss: 0.22037, val_acc: 0.21918\n",
      "Epoch [25/30], loss: 0.21040 acc: 0.18276 val_loss: 0.22096, val_acc: 0.19178\n",
      "Epoch [26/30], loss: 0.21042 acc: 0.17241 val_loss: 0.22096, val_acc: 0.19178\n",
      "Epoch [27/30], loss: 0.20947 acc: 0.17931 val_loss: 0.22056, val_acc: 0.21918\n",
      "Epoch [28/30], loss: 0.21021 acc: 0.16552 val_loss: 0.21975, val_acc: 0.20548\n",
      "Epoch [29/30], loss: 0.20637 acc: 0.17241 val_loss: 0.22072, val_acc: 0.21918\n",
      "Epoch [30/30], loss: 0.20957 acc: 0.17931 val_loss: 0.22099, val_acc: 0.21918\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "  train_loss = 0\n",
    "  train_acc = 0\n",
    "  val_loss = 0\n",
    "  val_acc = 0\n",
    "\n",
    "  #学習\n",
    "  model.train()\n",
    "\n",
    "  for images, labels in train_dataLoader:\n",
    "\n",
    "    #勾配の初期化(ループの頭でやる必要あり)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 訓練データの準備\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # 順伝搬計算\n",
    "    outputs = model(images)\n",
    "\n",
    "    # 誤差計算\n",
    "    loss = criterion(outputs, labels)\n",
    "    train_loss += loss.item()\n",
    "\n",
    "    # 学習\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #予測値算出\n",
    "    predicted = outputs.max(1)[1]\n",
    "\n",
    "    #正解件数算出\n",
    "    train_acc += (predicted == labels).sum()\n",
    "\n",
    "  # 訓練データに対する損失と精度の計算\n",
    "  avg_train_loss = train_loss / len(train_dataLoader.dataset)\n",
    "  avg_train_acc = train_acc / len(train_dataLoader.dataset)\n",
    "\n",
    "  #評価\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "\n",
    "    for images, labels in val_dataLoader:\n",
    "\n",
    "      # テストデータの準備\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      # 順伝搬計算\n",
    "      outputs = model(images)\n",
    "\n",
    "      # 誤差計算\n",
    "      loss = criterion(outputs, labels)\n",
    "      val_loss += loss.item()\n",
    "\n",
    "      #予測値算出\n",
    "      predicted = outputs.max(1)[1]\n",
    "\n",
    "      #正解件数算出\n",
    "      val_acc += (predicted == labels).sum()\n",
    "\n",
    "    # 検証データに対する損失と精度の計算\n",
    "    avg_val_loss = val_loss / len(val_dataLoader.dataset)\n",
    "    avg_val_acc = val_acc / len(val_dataLoader.dataset)\n",
    "\n",
    "  print (f'Epoch [{(i+1)}/{epoch}], loss: {avg_train_loss:.5f} acc: {avg_train_acc:.5f} val_loss: {avg_val_loss:.5f}, val_acc: {avg_val_acc:.5f}')\n",
    "  train_loss_list.append(avg_train_loss)\n",
    "  train_acc_list.append(avg_train_acc)\n",
    "  val_loss_list.append(avg_val_loss)\n",
    "  val_acc_list.append(avg_val_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "509ebe440ff8829e001a14a6a100e0bf6e98d8654d4df1b14b1a4d7c455d9242"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
